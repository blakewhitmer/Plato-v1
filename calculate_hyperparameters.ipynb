{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I used OpenAI's Scaling Laws paper to calculate hyperparameters for my model. However, the numbers generated at the end didn't pass the sniff test. The scaling laws told me that I should train my model for 4.28e+21 days--that's a lot of days. This motivated me to instead calculate hyperparameters using the Chinchilla paper.\n",
    "\n",
    "You should read this file if you want to see my thought process. My actual calculations occur in scalinglaws.py. I didn't want to redo the write up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to train an encoder-only transformer language model using MLX on my Macbook Pro M2. In this file, I am going to calculate the optimal hyperparameters according to OpenAI's Scaling Laws paper.\n",
    "\n",
    "See https://arxiv.org/pdf/2001.08361\n",
    "\n",
    "In this paper, OpenAI investigates the optimal ratio between model parameter count, dataset size, and minimum compute. They find that each of these three parameters must be held in the following proportion:\n",
    "\n",
    "D ‚àù N^0.74 ‚àù C^0.54\n",
    "\n",
    "As long as none of the parameters are a bottlenecked, increasing the scale of your model will reliably decrease the model's loss function.\n",
    "\n",
    "OpenAI assumes that you start with a fixed compute budget. However, I suspect that dataset size will be my limiting factor, so I will start with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens\n",
      "73584077\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "\n",
    "def concatenate_txt_files(directory):\n",
    "    combined_text = \"\"\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Attempt to read the file with utf-8 encoding\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        combined_text += f.read() + \"\\n\\n\"\n",
    "                \n",
    "                # If a UnicodeDecodeError occurs, try another encoding\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                            combined_text += f.read() + \"\\n\\n\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to read {file_path} due to {str(e)}\")\n",
    "\n",
    "    return combined_text\n",
    "\n",
    "directory = \"/Users/blakewhitmer/PlatoV1/PlatoV1Dataset/JustBooks\"\n",
    "combined_text = concatenate_txt_files(directory)\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = encoding.encode(combined_text, allowed_special=\"all\")\n",
    "\n",
    "print(\"Number of tokens\")\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset started much larger. I begain by putting Python code, math practice problems, and a smattering of ebooks together. But as I looked at initial generations after 10,000 or so iterations, they didn't look right. I thought my code was generating utter nonsense. So I cut out a lot. I first cut out the Project Gutenberg licenses. Don't worry, you can find them with my dataset on Hugging Face. But they were repeat data, and that really harms small models:\n",
    "\n",
    "https://www.anthropic.com/research/scaling-laws-and-interpretability-of-learning-from-repeated-data\n",
    "\n",
    "I also took out the math and Python code. I think having code, LaTeX, and parts of the Western Canon in Greek, German, and English would have been too much.\n",
    "\n",
    "I also skimmed through the dataset and cut out anything that wasn't natural language. Did you know Project Gutenberg has an ebook that's just pi to a million digits? I tried to include the first 200 Project Gutenberg ebooks, just as a heuristic, but it seems that was a mistake.\n",
    "\n",
    "You can see examples of some of the scripts I used to trim down my dataset in trimmingcanon.py. This was done almost entirely by intuition. If it didn't look like philosophy, I deleted it.\n",
    "\n",
    "Now, I need to calculate optimal parameters for my model. The math for this is extremely simple: it's a simple ratio of N^0.74/D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27e+10\n"
     ]
    }
   ],
   "source": [
    "paramter_count = 73584077 ** (1 / 0.74)\n",
    "print(format(paramter_count, \".2e\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the optimal parameter count is around 40 million parameters. And the optimal minimum compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.86e+19\n"
     ]
    }
   ],
   "source": [
    "optimal_compute = paramter_count ** (1 / 0.54)\n",
    "print(format(optimal_compute, \".2e\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to make sure this can actually fit within my compute budget! The Scaling Laws paper measures optimal compute in pedaflop-days. And this is the only resource I could find on the throughput of my Mac's GPU:\n",
    "\n",
    "https://www.cpu-monkey.com/en/igpu-apple_m2_pro_16_core\n",
    "\n",
    "That is 11.36 tflops for fp16.\n",
    "\n",
    "I will train the model for at least 1 day.\n",
    "\n",
    "I will try to train it for about 7 days. But as time goes on, if my macbook doesn't have anything better to do, I might leave it training for longer. And so putting that all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.81504e+17\n",
      "Total training time in days: 4.28e+21\n"
     ]
    }
   ],
   "source": [
    "pflop_day = 10 ** 15 * 60 * 60 * 24\n",
    "total_fp_operations_needed = optimal_compute * pflop_day\n",
    "fp_per_day = 11.36 * 10 ** 12 * 24 * 60 * 60\n",
    "print(fp_per_day)\n",
    "\n",
    "optimal_compute_in_days = total_fp_operations_needed / fp_per_day\n",
    "\n",
    "\n",
    "print(\"Total training time in days: \" + format(optimal_compute_in_days, \".2e\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of days. Maybe I should have started with my compute budget, like the scaling laws paper recommended. üò¨\n",
    "\n",
    "I hope to train my model somewhere in between 3 days and a week. I might train it more as time goes on. This would give me a compute budget, model size, and dataset size of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6870.528\n",
      "118.02517729411888\n",
      "34.13982176924179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_compute_budget = fp_per_day / 10 ** 15 * 7\n",
    "optimal_parameters = total_compute_budget ** 0.54\n",
    "minimum_dataset = optimal_parameters ** 0.74\n",
    "\n",
    "print(total_compute_budget)\n",
    "print(optimal_parameters)\n",
    "print(minimum_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... this doesn't look right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
